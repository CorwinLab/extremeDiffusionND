import numpy as np
import os
import glob
import h5py
import json
from memEfficientEvolve2DLattice import getExpVarXDotProduct

# moments calculation for files saved as .h5
def getStatsh5py(path,tCutOff=None,takeLog=True):
    """
    Calculates mean, variance, etc. of ln[Probability outside sphere]
    Parameters
    ----------
    path: str,  something like "data/memoryEfficientMeasurements/h5data/dirichlet/ALPHA3/L1000/tMax2000"
    tCutOff: optional, integer. time at which you want to cut off data before taking stats
    takeLog: boolean, default true.

    Returns
    -------
    saves Stats.h5 to the path given as a parameter

    """
    with open(f"{path}/variables.json",'r') as v:
        variables = json.load(v)
    time = np.array(variables['ts'])
    # TODO: fix this once I push the other thing, to fix the range issue
    # maxTime = time[-2]
    maxTime = time[-1] -1  # because of the range issue?
    print(maxTime)
    files = glob.glob(f"{path}/*.h5")
    # print(f"files: {files}")
    # TODO: add string option for if takeLog=False
    statsFile = h5py.File(f"{path}/Stats.h5",'a')
    moments = ['mean','secondMoment','var','skew']
    with h5py.File(files[0], 'r') as f:
        for regime in f['regimes'].keys():
            statsFile.require_group(regime)
            for moment in moments:
                statsFile[regime].require_dataset(moment, shape=f['regimes'][regime].shape, dtype=float)
                statsFile[regime][moment][:] = np.zeros(f['regimes'][regime].shape, dtype=float)
    num_files = 0
    for file in files:
        # print(file)
        with h5py.File(file, 'r') as f:
            if f.attrs['currentOccupancyTime'] < maxTime:
                print(f"file: {file}")
                print("Skipping")
                continue
            for regime in f['regimes'].keys():
                probs = f['regimes'][regime][:].astype(float)
                # TODO: figure out how to print something if this throws the error
                assert np.sum(np.isnan(probs)) == 0

                statsFile[regime]['mean'][:] += np.log(probs)
                statsFile[regime]['secondMoment'][:] += np.log(probs) ** 2
                # debugging prints
                # print(f"file: {file} \n regime: {regime}")
                # print(f"mean (unnormalized): {statsFile[regime]['mean'][:]}")
            num_files += 1
    for regime in statsFile.keys():
        statsFile[regime]['mean'][:] /= num_files
        statsFile[regime]['secondMoment'][:] /= num_files
        statsFile[regime]['var'][:] = statsFile[regime]['secondMoment'][:] - statsFile[regime]['mean'][:] ** 2


# inherently dependent on the way we save data but thats ok
def processStats(statsFile):
    """
    takes a h5 file with the saved, calculated stats of systems
    and outputs an array of [var(lnP), radius, time, lambda_ext ]
    that array should then get thrown into the calculation of s(r,t,lambda_ext)
    params: file path to an stats file generated by getStatsh5py
    returns:
        data: np array where 0th axis gives varLnP, radii, t, lambda_ext
    """
    stats = h5py.File(f"{statsFile}","r")
    # setup for grabbing list of vs and ts
    filePath = os.path.split(statsFile)[0]  # returns the directory data & stats in
    regimes = ['linear','tOnSqrtLogT','sqrt']
    testFile = h5py.File(f"{filePath}/0.h5","r")
    with open (f"{filePath}/variables.json","r") as v:
        variables = json.load(v)
    ts = np.array(variables['ts'])  # in order from 0 to 99999
    vs = np.array(variables['velocities'])
    # calculation of lambda_ext
    distName = variables['distName']
    params = np.array(variables['params'])
    if '' in params:
        params = ''
    expVarX = getExpVarXDotProduct(distName, params)
    # reshaping of arrays
    longTs = np.tile(ts,vs.shape[0])  # turn ts array into 1d array of size (336 * 221)
    # turn lambda_ext into 1d array of size (336*21 = 7056)
    lambda_ext = np.array([(expVarX / (1 - expVarX))]*(ts.shape[0]*vs.shape[0]))

    # fence post problem
    firstRadii = testFile['regimes'][regimes[0]].attrs['radii'].flatten('F')
    linearVLP = stats[regimes[0]]['var'][:].flatten('F')
    data = np.array([linearVLP,firstRadii,longTs,lambda_ext])
    if distName == 'Dirichlet':
        label = distName + str(params[0])
    else:
        label = distName
    for regime in regimes[1:]:
        # (336*21 = 7056 1d array) of linear radii
        # flatten in column major so its like (v[0] radii for all t, v[1] radii for all t, ... etc)
        radii = testFile['regimes'][regime].attrs['radii'].flatten(order='F')
        # returns (331*21 = 7056 1d array)
        varLnP = stats[regime]['var'][:].flatten(order='F')
        tempData = np.array([varLnP,radii,longTs,lambda_ext])
        data = np.hstack((data,tempData))
    # indices for data: (4, t.shape * v.shape)
    # the 0th index: 0 = VLP, 1 = radii, 2 = t, 3 = lambda
    # note that if you want to pull out a specific val, such as tMax
    # indices = np.where(data[2,:] == tmax)
    return data, label

def productOfDirichletNumbers(n):
    """" calculate the product of n dirichlet numbers"""
    params = np.array([0.1]*4)
    rand_vals = np.random.dirichlet(params,size=n)
    rand_vals = rand_vals.astype(np.quad)
    prod = np.prod(rand_vals[:,0])
    return prod

def getLogP(t):
    """ """
    sumP = (productOfDirichletNumbers(t) + productOfDirichletNumbers(t)
           + productOfDirichletNumbers(t) + productOfDirichletNumbers(t))
    logP = np.log(sumP)
    return logP.astype(float)

def diamondCornerVariance(t):
    """ calculate var[lnP] of rwre being at the 4 corners
    process (equiv of setting v=1 for vt regime):
    take product of t dirichlet numbers, 4 times independently
    repeat a ton of times
    then take log of all of them and calculate variance
    note because log(product) is sum(log) we can add instead?
    for now only going to do this for dirichlet as a check.
    which means lambda_ext will be for dirichlet with alpha=0.1
    """
    num_samples = 100000
    logPs = []
    for _ in range(num_samples):
        logPs.append(getLogP(t))
    var = np.var(logPs)
    return var


# for the var[lnP]|tmax, we want r(tmax) and tmax...
# but in theory any t should work.
def masterCurveValue(radii, times, lambda_ext):
    """
    note that if you want specific vals for some tMax you do like
    vals = masterCurveValue(data[1,:][indices],data[2,:][indices],data[3,:][indices])
    plt.loglog(vals,data[0,:][indices],'.') or something
    Parameters
    ----------
    radius: list of radii
    times: list of times
    lambda_ext: list of lambda_ext
    Returns
    -------
    f(r(t),t,lambda_ext) = (lambda_ext / 4pi) * (ln t / t^2) * r(t)^2
    """
    scalingFunction = (lambda_ext / (4*np.pi)) * (np.log(times) / times**2) * (radii **2)
    return scalingFunction

def getListOfLambdas(statsList):
    expVarXList = []
    lambdaList = []
    for path in statsList:
        filePath = os.path.split(path)[0]  # returns the directory data & stats in
        with open(f"{filePath}/variables.json", "r") as v:
            variables = json.load(v)
        # calculation of lambda_ext
        distName = variables['distName']
        params = np.array(variables['params'])
        if '' in params:
            params = ''
        expVarX = getExpVarXDotProduct(distName, params)
        expVarXList.append(expVarX)
        lambdaList.append((expVarX / (1 - expVarX)))
    return np.array(expVarXList), np.array(lambdaList)

def diamondVarFinal(ts):
    radii = ts
    params = np.array([0.1]*4)
    lambda_ext = getExpVarXDotProduct("Dirichlet",params)
    varLnPs = []
    for t in ts:
        varLnPs.append(diamondCornerVariance(t))
    return masterCurveValue(radii,ts,lambda_ext), varLnPs